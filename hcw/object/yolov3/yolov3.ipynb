{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.11.1-py3-none-any.whl (285 kB)\n",
      "\u001b[K     |████████████████████████████████| 285 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.7/site-packages (from seaborn) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.7/site-packages (from seaborn) (1.18.5)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.7/site-packages (from seaborn) (3.4.2)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.7/site-packages (from seaborn) (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (7.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23->seaborn) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.14.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.11.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "xw7Cnp2PuAWx",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "8e01d56c-7de3-47bb-c9b0-767eaca55ff6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov3'...\n",
      "remote: Enumerating objects: 9783, done.\u001b[K\n",
      "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
      "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
      "remote: Total 9783 (delta 0), reused 2 (delta 0), pack-reused 9779\u001b[K\n",
      "Receiving objects: 100% (9783/9783), 8.80 MiB | 7.43 MiB/s, done.\n",
      "Resolving deltas: 100% (6656/6656), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov3.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Fs36TVH3uAW_"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('./yolov3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6A8ZqlakuAXB"
   },
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from utils.loss import ComputeLoss\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "from models.yolo import Model\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from utils.general import coco80_to_coco91_class, check_dataset, check_file, check_img_size, check_requirements, \\\n",
    "    box_iou, non_max_suppression, scale_coords, xyxy2xywh, xywh2xyxy, set_logging, increment_path, colorstr\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Iasidk5TuAXD"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    '''\n",
    "      data_dir: data가 존재하는 폴더 경로\n",
    "      transforms: data transform (resize, crop, Totensor, etc,,,)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, annotation, data_dir, transforms=None, image_size=512):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.coco = COCO(annotation)\n",
    "        self.image_size = image_size\n",
    "        self.predictions = {\n",
    "            \"images\": self.coco.dataset[\"images\"].copy(),\n",
    "            \"categories\": self.coco.dataset[\"categories\"].copy(),\n",
    "            \"annotations\": None\n",
    "        }\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # boxes (x, y, w, h)\n",
    "        boxes = np.array([x['bbox'] for x in anns])\n",
    "\n",
    "        # boxes (x_min, y_min, x_max, y_max)\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        boxes[:, 0] /= int(self.image_size)\n",
    "        boxes[:, 1] /= int(self.image_size)\n",
    "        boxes[:, 2] /= int(self.image_size)\n",
    "        boxes[:, 3] /= int(self.image_size)\n",
    "        \n",
    "        labels = list([x['category_id'] for x in anns])\n",
    "        areas = np.array([x['area'] for x in anns])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        \n",
    "        is_crowds = np.array([x['iscrowd'] for x in anns])\n",
    "        is_crowds = torch.as_tensor(is_crowds, dtype=torch.int64)\n",
    "        \n",
    "        segmentation = np.array([x['segmentation'] for x in anns], dtype=object)\n",
    "\n",
    "        \n",
    "        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([index]), 'area': areas,\n",
    "                  'iscrowd': is_crowds}\n",
    "\n",
    "        # transform\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            target['boxes'] = [list(box) for box in sample['bboxes']]\n",
    "\n",
    "        result = []\n",
    "        labels_out = torch.zeros((len(labels), 6))\n",
    "        for i in range(len(labels)):\n",
    "            result.append(\n",
    "                [labels[i], target['boxes'][i][0], target['boxes'][i][1], target['boxes'][i][2], target['boxes'][i][3]])\n",
    "        result = np.array(result)\n",
    "\n",
    "        if len(labels):\n",
    "            labels_out[:, 1:] = torch.from_numpy(result)\n",
    "\n",
    "        return image, labels_out, image_id\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "a5NdvXCruAXH"
   },
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(512, 512),\n",
    "        A.Flip(p=0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AfVhGLR_uAXI"
   },
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    img, label, image_id = zip(*batch)  # transposed\n",
    "    for i, l in enumerate(label):\n",
    "        l[:, 0] = i  # add target image index for build_targets()\n",
    "    return torch.stack(img, 0), torch.cat(label, 0), image_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yXKbrOneuAXK"
   },
   "outputs": [],
   "source": [
    "def train_fn(num_epochs, train_data_loader, optimizer, model, device):\n",
    "    model.gr = 1.0\n",
    "    itr = 1\n",
    "    loss_hist = Averager()\n",
    "    compute_loss = ComputeLoss(model)\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_hist.reset()\n",
    "\n",
    "        for images, targets, image_ids in tqdm(train_data_loader):\n",
    "            images = torch.as_tensor([image.numpy() for image in images]).to(device)\n",
    "            pred = model(images)\n",
    "\n",
    "            losses, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size\n",
    "\n",
    "            loss_value = losses.item()\n",
    "\n",
    "            loss_hist.send(loss_value)\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        print(f\"Epoch #{epoch} loss: {loss_hist.value}\")\n",
    "        torch.save(model.state_dict(), f'./yolov3/weights/epoch_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "XndvII9XuAXM"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    data_dir = '../../input/data'\n",
    "    annotation = '../../input/data/train.json'\n",
    "    train_dataset = CustomDataset(annotation, data_dir, get_train_transform())\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # load a model;\n",
    "\n",
    "    num_classes = 11\n",
    "    model = Model(cfg='./yolov3/models/yolov3.yaml', ch=3, nc=num_classes)  # create\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    model.to(device)\n",
    "\n",
    "    # Hyperparameters\n",
    "    with open('./yolov3/data/hyp.scratch.yaml') as f:\n",
    "        hyp = yaml.load(f, Loader=yaml.SafeLoader)  # load hyps\n",
    "    model.hyp = hyp\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    num_epochs = 12\n",
    "\n",
    "    train_fn(num_epochs, train_data_loader, optimizer, model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "qk3ekrF_uAXN",
    "outputId": "a94c670d-6bef-4253-e0aa-a68710f5d298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.72s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 655/655 [05:56<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 loss: 1.1051222209740459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 655/655 [05:56<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 loss: 1.0143869513681314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 655/655 [05:53<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2 loss: 1.036216577665724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 655/655 [05:58<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3 loss: 1.0040201663899855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 655/655 [06:03<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4 loss: 1.0148993356224463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 655/655 [05:58<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5 loss: 0.9866717331879239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 655/655 [05:59<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6 loss: 0.9863667491411093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 655/655 [05:59<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7 loss: 0.9946460844081549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 655/655 [05:58<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8 loss: 1.0022297453121034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 655/655 [06:01<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9 loss: 1.0011018390319388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 655/655 [06:02<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10 loss: 0.9625316589018543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 655/655 [06:02<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11 loss: 0.9770777327186279\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/code/mmdetection_trash\n"
     ]
    }
   ],
   "source": [
    "cd mmdetection_trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "yolov3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
