{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eda6dea4-5aab-476d-9f8e-be4553ef2969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/qubvel/segmentation_models.pytorch\n",
      "  Cloning https://github.com/qubvel/segmentation_models.pytorch to /tmp/pip-req-build-is0tu_ex\n",
      "  Running command git clone -q https://github.com/qubvel/segmentation_models.pytorch /tmp/pip-req-build-is0tu_ex\n",
      "Requirement already satisfied (use --upgrade to upgrade): segmentation-models-pytorch==0.1.3 from git+https://github.com/qubvel/segmentation_models.pytorch in /opt/conda/lib/python3.7/site-packages\n",
      "Requirement already satisfied: torchvision>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch==0.1.3) (0.5.0)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch==0.1.3) (0.7.4)\n",
      "Requirement already satisfied: efficientnet-pytorch==0.6.3 in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch==0.1.3) (0.6.3)\n",
      "Requirement already satisfied: timm==0.3.2 in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch==0.1.3) (0.3.2)\n",
      "Requirement already satisfied: torch==1.4.0 in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.3) (1.4.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.3) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.3) (7.2.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.3) (1.19.5)\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.1.3) (2.5.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.1.3) (4.60.0)\n",
      "Building wheels for collected packages: segmentation-models-pytorch\n",
      "  Building wheel for segmentation-models-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for segmentation-models-pytorch: filename=segmentation_models_pytorch-0.1.3-py3-none-any.whl size=83164 sha256=828b1a73f6c1871a6366f72629ea31656a5d6cd2ca37823b7483125de4139f17\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cxn59cr_/wheels/fa/c5/a8/1e8af6cb04a0974db8a4a156ebd2fdd1d99ad2558d3fce49d4\n",
      "Successfully built segmentation-models-pytorch\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/qubvel/segmentation_models.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38e1b8b9-f310-4ae7-a081-24ea2725e556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.4.0\n",
      "GPU 사용 가능 여부: True\n",
      "Tesla P40\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import label_accuracy_score\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 전처리를 위한 라이브러리\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af605b7f-8d42-4921-8022-a9d10c7c655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16   # Mini-batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3bfb89e-4490-4def-9de6-b0df24545984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of super categories: 11\n",
      "Number of categories: 11\n",
      "Number of annotations: 21116\n",
      "Number of images: 2617\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "dataset_path = '../input/data'\n",
    "anns_file_path = dataset_path + '/' + 'train.json'\n",
    "\n",
    "# Read annotations\n",
    "with open(anns_file_path, 'r') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "\n",
    "categories = dataset['categories']\n",
    "anns = dataset['annotations']\n",
    "imgs = dataset['images']\n",
    "nr_cats = len(categories)\n",
    "nr_annotations = len(anns)\n",
    "nr_images = len(imgs)\n",
    "\n",
    "# Load categories and super categories\n",
    "cat_names = []\n",
    "super_cat_names = []\n",
    "super_cat_ids = {}\n",
    "super_cat_last_name = ''\n",
    "nr_super_cats = 0\n",
    "for cat_it in categories:\n",
    "    cat_names.append(cat_it['name'])\n",
    "    super_cat_name = cat_it['supercategory']\n",
    "    # Adding new supercat\n",
    "    if super_cat_name != super_cat_last_name:\n",
    "        super_cat_names.append(super_cat_name)\n",
    "        super_cat_ids[super_cat_name] = nr_super_cats\n",
    "        super_cat_last_name = super_cat_name\n",
    "        nr_super_cats += 1\n",
    "\n",
    "print('Number of super categories:', nr_super_cats)\n",
    "print('Number of categories:', nr_cats)\n",
    "print('Number of annotations:', nr_annotations)\n",
    "print('Number of images:', nr_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a391127-5326-4bf7-9f9f-a14beda170b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count annotations\n",
    "cat_histogram = np.zeros(nr_cats,dtype=int)\n",
    "for ann in anns:\n",
    "    cat_histogram[ann['category_id']] += 1\n",
    "\n",
    "# Initialize the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({'Categories': cat_names, 'Number of annotations': cat_histogram})\n",
    "df = df.sort_values('Number of annotations', 0, False)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.title(\"category distribution of train set \")\n",
    "plot_1 = sns.barplot(x=\"Number of annotations\", y=\"Categories\", data=df, label=\"Total\", color=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "848a480f-58f8-4830-b75f-76f30d426d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted_temp_df = df.sort_index()\n",
    "sorted_df = pd.DataFrame([\"Backgroud\"], columns = [\"Categories\"])\n",
    "sorted_df = sorted_df.append(sorted_temp_df, ignore_index=True)\n",
    "category_names = list(sorted_df.Categories)\n",
    "\n",
    "def get_classname(classID, cats):\n",
    "    for i in range(len(cats)):\n",
    "        if cats[i]['id']==classID:\n",
    "            return cats[i]['name']\n",
    "    return \"None\"\n",
    "\n",
    "class CustomDataLoader(Dataset):\n",
    "    \"\"\"COCO format\"\"\"\n",
    "    def __init__(self, data_dir, mode = 'train', transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        # dataset이 index되어 list처럼 동작\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_infos = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        # cv2 를 활용하여 image 불러오기\n",
    "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.uint8)\n",
    "        \n",
    "        if (self.mode in ('train', 'val')):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Load the categories in a variable\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            # masks : size가 (height x width)인 2D\n",
    "            # 각각의 pixel 값에는 \"category id + 1\" 할당\n",
    "            # Background = 0\n",
    "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "            # Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
    "            for i in range(len(anns)):\n",
    "                className = get_classname(anns[i]['category_id'], cats)\n",
    "                pixel_value = category_names.index(className)\n",
    "                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n",
    "            masks = masks.astype(np.uint8)\n",
    "\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images, mask=masks)\n",
    "                images = transformed[\"image\"]\n",
    "                masks = transformed[\"mask\"]\n",
    "                \n",
    "            images = images.type(torch.float32)\n",
    "            \n",
    "            return images, masks, image_infos\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "                \n",
    "            images = images.type(torch.float32)\n",
    "            \n",
    "            return images, image_infos\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # 전체 dataset의 size를 return\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0267dca-a2ef-4427-9536-058ee1f27978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# train.json / validation.json / test.json 디렉토리 설정\n",
    "train_path = dataset_path + '/train.json'\n",
    "val_path = dataset_path + '/val.json'\n",
    "test_path = dataset_path + '/test.json'\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                            A.Normalize(\n",
    "                                mean=(0.485, 0.456, 0.406),\n",
    "                                std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0\n",
    "                            ),    \n",
    "                        ToTensorV2(transpose_mask=True)\n",
    "        ])\n",
    "\n",
    "test_dataset = CustomDataLoader(data_dir=test_path, mode='test', transform=test_transform)\n",
    "\n",
    "# DataLoader\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          num_workers=2,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6272b77-a5f5-4089-8bc0-7e4f6ab50d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_test(models, test_loader, device):\n",
    "    size = 256\n",
    "    transform = A.Compose([A.Resize(256, 256)])\n",
    "    print('Start prediction.')\n",
    "    for i in range(len(models)) :\n",
    "        models[i].eval()\n",
    "    \n",
    "    file_name_list = []\n",
    "    preds_array = np.empty((0, size*size), dtype=np.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (imgs, image_infos) in enumerate(test_loader):\n",
    "            outs_list = []\n",
    "\n",
    "            # inference (512 x 512)\n",
    "            for i in range(len(models)):\n",
    "                out = models[i](torch.stack(imgs).to(device))\n",
    "                outs_list.append(out)\n",
    "            \n",
    "            outs = outs_list[0]\n",
    "            for i in range(1, len(outs_list)):\n",
    "                outs += outs_list[i]\n",
    "                \n",
    "            oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "            \n",
    "            # resize (256 x 256)\n",
    "            temp_mask = []\n",
    "            for img, mask in zip(np.stack(imgs), oms):\n",
    "                transformed = transform(image=img, mask=mask)\n",
    "                mask = transformed['mask']\n",
    "                temp_mask.append(mask)\n",
    "\n",
    "            oms = np.array(temp_mask)\n",
    "            \n",
    "            oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n",
    "            preds_array = np.vstack((preds_array, oms))\n",
    "            \n",
    "            file_name_list.append([i['file_name'] for i in image_infos])\n",
    "    print(\"End prediction.\")\n",
    "    file_names = [y for x in file_name_list for y in x]\n",
    "    \n",
    "    return file_names, preds_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fd41594-c166-400a-9be1-a863a6b36be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"se_resnext101_32x4d\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=12,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "model2 = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"se_resnext101_32x4d\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=12,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "model3 = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"se_resnext101_32x4d\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=12,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "model4 = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"se_resnext101_32x4d\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=12,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "model5 = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"se_resnext101_32x4d\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=12,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "model6 = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"se_resnext101_32x4d\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=12,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "model7 = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"senet154\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=12,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "model8 = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"se_resnext101_32x4d\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=12,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "model9 = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"senet154\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=12,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "model10 = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"efficientnet-b7\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=12,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "model11 = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"efficientnet-b7\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=12,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "model12 = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"efficientnet-b4\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=12,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "model13 = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"efficientnet-b4\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=12,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "# best model 저장된 경로\n",
    "model_path1 = './saved/fold1-2.pth'\n",
    "model_path2 = './saved/fold2-2.pt'\n",
    "model_path3 = './saved/fold3-2.pt'\n",
    "model_path4 = './saved/fold4-2.pt'\n",
    "model_path5 = './saved/fold5-1.pt'\n",
    "model_path6 = './saved/se_resnext_hcw.pth'\n",
    "model_path7 = './saved/senet154_cym.pt'\n",
    "model_path8 = './saved/se_resnet101_hcw.pth'\n",
    "model_path9 = './saved/senet154_cym2.pt'\n",
    "model_path10 = './saved/efficientnet-b7_ksh.pt'\n",
    "model_path11 = './saved/efficientnet-b7_ksh2.pt'\n",
    "model_path12 = './saved/efficientnet-b4_ksh.pt'\n",
    "model_path13 = './saved/efficientnet-b4_ksh2.pt'\n",
    "\n",
    "# best model 불러오기\n",
    "checkpoint1 = torch.load(model_path1, map_location=device)\n",
    "checkpoint2 = torch.load(model_path2, map_location=device)\n",
    "checkpoint3 = torch.load(model_path3, map_location=device)\n",
    "checkpoint4 = torch.load(model_path4, map_location=device)\n",
    "checkpoint5 = torch.load(model_path5, map_location=device)\n",
    "checkpoint6 = torch.load(model_path6, map_location=device)\n",
    "checkpoint7 = torch.load(model_path7, map_location=device)\n",
    "checkpoint8 = torch.load(model_path8, map_location=device)\n",
    "checkpoint9 = torch.load(model_path9, map_location=device)\n",
    "checkpoint10 = torch.load(model_path10, map_location=device)\n",
    "checkpoint11 = torch.load(model_path11, map_location=device)\n",
    "checkpoint12 = torch.load(model_path12, map_location=device)\n",
    "checkpoint13 = torch.load(model_path13, map_location=device)\n",
    "\n",
    "'''\n",
    "# DeepLab을 사용시에는 아래와 같이 사용 \n",
    "model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "model.classifier[-1] = nn.Conv2d(256, 12, kernel_size=1, stride=1)\n",
    "'''\n",
    "models = []\n",
    "\n",
    "model1.load_state_dict(checkpoint1)\n",
    "model1 = model1.to(device)\n",
    "models.append(model1)\n",
    "\n",
    "model2.load_state_dict(checkpoint2)\n",
    "model2 = model2.to(device)\n",
    "models.append(model2)\n",
    "\n",
    "model3.load_state_dict(checkpoint3)\n",
    "model3 = model3.to(device)\n",
    "models.append(model3)\n",
    "\n",
    "model4.load_state_dict(checkpoint4)\n",
    "model4 = model4.to(device)\n",
    "models.append(model4)\n",
    "\n",
    "model5.load_state_dict(checkpoint5)\n",
    "model5 = model5.to(device)\n",
    "models.append(model5)\n",
    "\n",
    "model6.load_state_dict(checkpoint6)\n",
    "model6 = model6.to(device)\n",
    "models.append(model6)\n",
    "\n",
    "model7.load_state_dict(checkpoint7)\n",
    "model7 = model7.to(device)\n",
    "models.append(model7)\n",
    "\n",
    "model8.load_state_dict(checkpoint8)\n",
    "model8 = model8.to(device)\n",
    "models.append(model8)\n",
    "\n",
    "model9.load_state_dict(checkpoint9)\n",
    "model9 = model9.to(device)\n",
    "models.append(model9)\n",
    "\n",
    "model10.load_state_dict(checkpoint10, strict=False)\n",
    "model10 = model10.to(device)\n",
    "models.append(model10)\n",
    "\n",
    "model11.load_state_dict(checkpoint11, strict=False)\n",
    "model11 = model11.to(device)\n",
    "models.append(model11)\n",
    "\n",
    "model12.load_state_dict(checkpoint12, strict=False)\n",
    "model12 = model12.to(device)\n",
    "models.append(model12)\n",
    "\n",
    "model13.load_state_dict(checkpoint13, strict=False)\n",
    "model13 = model13.to(device)\n",
    "models.append(model13)\n",
    "\n",
    "# 추론을 실행하기 전에는 반드시 설정 (batch normalization, dropout 를 평가 모드로 설정)\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d91a211e-fafd-4854-99b4-789ce34a8a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction.\n",
      "End prediction.\n"
     ]
    }
   ],
   "source": [
    "# sample_submisson.csv 열기\n",
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "# test set에 대한 prediction\n",
    "file_names, preds = ensemble_test(models, test_loader, device) ###########################\n",
    "\n",
    "# PredictionString 대입\n",
    "for file_name, string in zip(file_names, preds):\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                   ignore_index=True)\n",
    "\n",
    "# submission.csv로 저장\n",
    "submission.to_csv(\"./submission/ensemble.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c4e48-212f-4e11-b034-bd0064434eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7c024-5ad2-4db8-aed8-3ed6dbddf1af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a6809c-0d6b-486c-ab2a-f0adf03bc035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb623432-9c50-4d15-8d02-8c7a2b775520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
